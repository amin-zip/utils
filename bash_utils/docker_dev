#!/bin/bash
set -e

#******************************************************************************
#  Copyright (c) 2024 Zipline International, Inc.  All rights reserved.
#
# docker_dev creates a persistent docker container for user development. This is
# meant to enable us to use development environments generated by CI while
# isolating FlightSystems dependencies from the host system.
#
# This is similar to docker_bash, but the container is persistent and therefore
# reentrant.
#
# On Linux systems, docker uses no virtualization, so the only overhead cost of
# working within this system should really come from overlayfs. This concept
# should extend cleanly to MacOS (with some changes to support zsh and
# different device paths), but will likely incur cost from virtualization.
#===============================================================================


############ Sanity check environment ############

if [ -n "${REPO_PATH}" ]; then
    source "${REPO_PATH}"/sim/utils/utils.sh
else
    printf "ERROR: path to repository root is not defined.\n"
    printf "ERROR: please execute: source <repo root>/docker_fun.sh\n"
    exit 1
fi

# This script should not be run within a docker container.
if ! check_not_in_dockerenv; then
    exit 1
fi


############ Define variables ############

# Configure container with lenient default args for development. We really just want
# filesystem isolation, so make everything privileged.
docker_create_args=(
    --privileged
    --user "$(id -u):$(id -g)"
    -e DISPLAY="${DISPLAY}"
    -e QT_X11_NO_MITSHM=1
    -v "${HOME}"/.cache:"${HOME}"/.cache
    -v "${HOME}"/data:"${HOME}"/data
    -v "${HOME}"/.aws:"${HOME}"/.aws
    -v "${HOME}"/.ssh:"${HOME}"/.ssh
    -v /dev:/dev
    -v /run:/run
    -v /tmp/.X11-unix:/tmp/.X11-unix:rw
    -v "${HOME}"/.Xauthority:/root/.Xauthority:ro
    -v "${REPO_PATH}":"${REPO_PATH}"
    -it
    --device /dev/dri
    --workdir "${REPO_PATH}"
    --network host
    --entrypoint /bin/bash
)


############ Helper functions ############

# Dump usage information and exit.
function print_usage_and_exit() {
    echo "
Usage: docker_dev [options]

Start bash shell in a specified Docker image.

Options:
  down                              Kill the running container.
  delete                            Delete the running container and user image.
  rebuild                           Pull the latest build image and rebuild.
"
    exit 0
}

# Get the base image tag.
function base_image_tag() {
    if with_nvidia; then
        echo "zipline/flight-systems-cuda:latest"
    else
        echo "zipline/flight-systems-build:latest"
    fi
}

# Get the container name.
function container_name() {
    if with_nvidia; then
        echo "perception-cuda-dev"
    else
        echo "perception-dev"
    fi
}

# Get the user image tag. This is the image we'll build on top of the base
# image and tag specifically for the developer.
function user_image_tag() {
    echo "$(base_image_tag)-${USER}"
}

# Run any missing configuration required on Nvidia systems.
function maybe_install_nvidia_runtime() {
    # We'll use the nvidia container runtime to handle forwarding all cuda-specific
    # capabilities and nvidia drivers support for us.
    if [[ ! $(type -P nvidia-container-runtime) ]]; then
        print_warning "Nvidia container runtime not found."
        read -r -e -p " Install and configure? [Y/n]: " REPLY

        if [[ ${REPLY} == [Nn]*   ]]; then
            print_error "Configuration failed."
            exit 2
        fi

        # Configure the apt repository.
        curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | \
            sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg && \
        curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
            sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
            sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list > /dev/null
        # Now install the runtime.
        sudo apt update
        sudo apt install -y nvidia-container-runtime

        # Configure the docker daemon so we can optionally select the nvidia runtime.
        sudo tee /etc/docker/daemon.json > /dev/null <<EOF
{
    "runtimes": {
        "nvidia": {
            "path": "/usr/bin/nvidia-container-runtime",
            "runtimeArgs": []
        }
    }
}
EOF
        # Restart the docker service so the runtime is available as an option.
        sudo systemctl reload docker.service
        print_info "Nvidia runtime configured."
    fi
}

# Check if the host is running nvidia drivers.
function with_nvidia() {
    # This should be an adequate approximation. Just checking for the installation of the driver
    # isn't sufficient because the Nvidia driver may be installed, but the adapter not enabled.
    if [[ $(prime-select query) == "nvidia" ]]; then
        return 0
    else 
        return 1
    fi
}

# Set up Nvidia-specific things.
function configure_nvidia() {
    # Specify that we should use the nvidia runtime.
    docker_create_args+=(--runtime=nvidia)

    # Also install the runtime if we don't have it.
    maybe_install_nvidia_runtime
}

# Pull the base image that we'll build on top of.
function pull_base_image() {
    if with_nvidia; then
        docker_pull -c
    else
        docker_pull
    fi
}

# If we don't have the user image, build it here.
function maybe_build_user_image() {

    if [[ $(docker images -q "$(base_image_tag)" 2> /dev/null) == "" ]]; then
        print_info "Base image not found in local repository. Pulling now."
        pull_base_image
    fi

    # If the image doesn't exist...
    if [[ $(docker images -q "$(user_image_tag)" 2> /dev/null) == "" ]]; then
        # This introduces new layers on top of the base image provided by CI and
        # bakes in the user and group. We can also add layers for more dev tooling
        # here.
        # This really should be split out into a separate dockerfile and configured
        # with build-arg, but this is convenient for prototyping.
        print_info "User dev image not found. Building now."
        docker build -t "$(user_image_tag)" -<<EOF
FROM "$(base_image_tag)"

# Colorize the prompt.
ENV TERM xterm-256color

# Bake user and group ID into the image.
RUN groupadd --gid $(id --group) ${USER} && \
    useradd -l --uid ${UID} --gid $(id --group) -m ${USER}

# Modify the prompt so the user knows when they're in a container.
RUN echo "docker-dev" > /etc/debian_chroot

# Require no password for sudo access.
# This exposes a big security threat: when a user is added to the docker group,
# anyone with access to the machine can get root access to the filesystem through
# docker!
RUN echo "$USER ALL=(ALL) NOPASSWD:ALL" >> /etc/sudoers

# Install additional graphics libraries.
RUN apt update && apt install -y \
    mesa-utils \
    libgl1-mesa-glx \
    libxcursor-dev \
    libxrandr-dev \
    libxi-dev \
    && rm -rf /var/lib/apt/lists/*

# Install common development tools.
RUN apt update && apt install -y \
    vim \
    tmux \
    && rm -rf /var/lib/apt/lists/*
EOF
    fi
}

# Create the container if needed.
function maybe_create_dev_container() {
    if [[ $(docker ps -a -q -f name="$(container_name)" 2> /dev/null) == "" ]]; then
        print_info "No container found. Creating."
        docker create "${docker_create_args[@]}" -it --name "$(container_name)" "$(user_image_tag)" > /dev/null
    fi
}

# Start the container, then start a bash process within the container.
function start_container_and_enter() {
    # Start the container. No-op if already started.
    docker start "$(container_name)"
    # Now execute an interactive bash process.
    docker exec -it "$(container_name)" /bin/bash
}


############ Parsing and execution ############

# Default run options for this script.
OPTION="run"

# Parse user input.
while (($# > 0)); do
    case $1 in
         -h | --help)
            print_usage_and_exit
            ;;
         down | delete | rebuild)
            if [[ ${OPTION} != "run" ]]; then
                print_error "Only one option is supported."
                exit 2
            fi
            OPTION=${1}
            ;;
         *) print_error "Unrecognized argument: ${1}"
            print_usage_and_exit
            ;;
    esac
    shift
done

# Options requiring us to bring the container down.
case "${OPTION}" in down | delete | rebuild )
    print_info "Killing development container."
    docker stop "$(container_name)" 2> /dev/null || true
esac

# Options requiring us to delete the container.
case ${OPTION} in delete | rebuild )
    print_info "Removing development container."
    docker rm "$(container_name)" 2> /dev/null || true
    print_info "Removing user image."
    docker image rm "$(user_image_tag)" 2> /dev/null || true
esac

# Options requiring us to force pull
case $OPTION in rebuild )
    # Force pull a new base image to build on. 
    pull_base_image 
esac

# Options requiring us to build/run a new container.
case $OPTION in rebuild | run )
    # User just wants to attach to a container (that may or may not be created yet)
    # and do their thing.
    if with_nvidia; then
        configure_nvidia
    fi
    maybe_build_user_image
    maybe_create_dev_container
    start_container_and_enter
esac
