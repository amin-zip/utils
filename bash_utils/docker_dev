#!/bin/bash
set -e

#******************************************************************************
#  Copyright (c) 2024 Zipline International, Inc.  All rights reserved.
#
# docker_dev creates a persistent docker container for user development. This is
# meant to enable us to use development environments generated by CI while
# isolating FlightSystems dependencies from the host system.
#
# This is similar to docker_bash, but the container is persistent and therefore
# reentrant.
#
# On Linux systems, docker uses no virtualization, so the only overhead cost of
# working within this system should really come from overlayfs. This concept
# should extend cleanly to MacOS (with some changes to support zsh and
# different device paths), but will likely incur cost from virtualization.
#===============================================================================


############ Set paths ############

SCRIPT_DIR=$( cd -- "$( dirname -- "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )
FILES_DIR="${SCRIPT_DIR}"/docker_dev_files


############ Sanity check environment ############

if [ -n "${REPO_PATH}" ]; then
    source "${REPO_PATH}"/sim/utils/utils.sh
else
    printf "ERROR: path to repository root is not defined.\n"
    printf "ERROR: please execute: source <repo root>/docker_fun.sh\n"
    exit 1
fi

# This script should not be run within a docker container.
if ! check_not_in_dockerenv; then
    exit 1
fi


############ Define common docker create args ############

# Configure container with lenient default args for development. We really just want
# filesystem isolation, so make everything privileged.

# These arguments should be common among all developers. For arguments specific to one
# machine, see user_config.json.example.

docker_create_common_flags=(
    --privileged
    --user "$(id -u):$(id -g)"
    --workdir "${REPO_PATH}"
    --network host
    -it
    --entrypoint /bin/bash
)

docker_create_common_mounts=(
    -v "${HOME}"/.cache:"${HOME}"/.cache
    -v "${HOME}"/.aws:"${HOME}"/.aws
    -v "${HOME}"/.ssh:"${HOME}"/.ssh
    -v "${HOME}"/.Xauthority:/root/.Xauthority:ro
    -v "${REPO_PATH}":"${REPO_PATH}"
    -v /dev:/dev
    -v /run:/run
    -v /tmp/.X11-unix:/tmp/.X11-unix:rw
)

docker_create_common_devices=(
    --device /dev/dri
)

docker_create_common_env_vars=(
    -e QT_X11_NO_MITSHM=1
    -e DISPLAY="${DISPLAY}"
)

############ Helper functions ############

# Dump usage information and exit.
function print_usage_and_exit() {
    echo "
Usage: docker_dev [options]

Start bash shell in a specified Docker image.

Options:
  down                              Kill the running container.
  delete                            Delete the running container and user image.
  rebuild                           Pull the latest build image and rebuild.
"
    exit 0
}

# Install specified apt packages if missing
function maybe_apt_install() {
    for package in "$@"; do
        if ! dpkg -s "$package" > /dev/null 2>&1; then
            print_info "Installing $package"
            sudo apt install "$package" -y
        fi
    done
}

# Get the base image tag.
function base_image_tag() {
    if with_nvidia; then
        echo "zipline/flight-systems-cuda:latest"
    else
        echo "zipline/flight-systems-build:latest"
    fi
}

# Get the container name.
function container_name() {
    if with_nvidia; then
        echo "perception-cuda-dev"
    else
        echo "perception-dev"
    fi
}

# Get the user image tag. This is the image we'll build on top of the base
# image and tag specifically for the developer.
function user_image_tag() {
    echo "$(base_image_tag)-${USER}"
}

# Run any missing configuration required on Nvidia systems.
function maybe_install_nvidia_runtime() {
    # We'll use the nvidia container runtime to handle forwarding all cuda-specific
    # capabilities and nvidia drivers support for us.
    if [[ ! $(type -P nvidia-container-runtime) ]]; then
        print_warning "Nvidia container runtime not found."
        read -r -e -p " Install and configure? [Y/n]: " REPLY

        if [[ ${REPLY} == [Nn]*   ]]; then
            print_error "Configuration failed."
            exit 2
        fi

        # Configure the apt repository.
        curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | \
            sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg && \
        curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
            sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
            sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list > /dev/null
        # Now install the runtime.
        sudo apt update
        sudo apt install -y nvidia-container-runtime

        # Configure the docker daemon so we can optionally select the nvidia runtime.
        sudo tee /etc/docker/daemon.json > /dev/null <<EOF
{
    "runtimes": {
        "nvidia": {
            "path": "/usr/bin/nvidia-container-runtime",
            "runtimeArgs": []
        }
    }
}
EOF
        # Restart the docker service so the runtime is available as an option.
        sudo systemctl reload docker.service
        print_info "Nvidia runtime configured."
    fi
}

# Check if the host is running nvidia drivers.
function with_nvidia() {
    # Use the presence of nvidia-smi as an indicator that we're running on an Nvidia system. This can cause
    # things to break on systems with Nvidia cards, but where prime-select indicates that another adaptor
    # is being used.
    if [[ $(type -P nvidia-smi)  ]]; then
        return 0
    else 
        return 1
    fi
}

# Set up Nvidia-specific things.
function configure_nvidia() {
    # Specify that we should use the nvidia runtime.
    docker_create_common_flags+=(--runtime=nvidia)

    # Also install the runtime if we don't have it.
    maybe_install_nvidia_runtime
}

# Pull the base image that we'll build on top of.
function pull_base_image() {
    if with_nvidia; then
        docker_pull -c
    else
        docker_pull
    fi
}

# If we don't have the user image, build it here.
function maybe_build_user_image() {

    if [[ $(docker images -q "$(base_image_tag)" 2> /dev/null) == "" ]]; then
        print_info "Base image not found in local repository. Pulling now."
        pull_base_image
    fi

    # If the image doesn't exist...
    if [[ $(docker images -q "$(user_image_tag)" 2> /dev/null) == "" ]]; then
        # This introduces new layers on top of the base image provided by CI and
        # bakes in the user and group and some graphics libraries.
        print_info "User dev image not found. Building now."
        docker build \
            --build-arg BASE_IMAGE="$(base_image_tag)" \
            --build-arg USER="${USER}" \
            --build-arg UID="${UID}" \
            --build-arg GID="$(id --group)" \
            -t "$(user_image_tag)" \
            -f "${FILES_DIR}/dockerfile.base" \
            "${FILES_DIR}"

        # If there is a user-defined dockerfile, build a new image by appending that
        # dockerfile on top of the user base.
        if [ -f "${FILES_DIR}/dockerfile.user" ]; then
            print_info "Custom dockerfile found. Creating custom image."

            # Retag the image we just built as a base.
            docker tag "$(user_image_tag)" "$(user_image_tag)"-base

            # Now build the user layer.
            docker build \
                --build-arg BASE_IMAGE="$(user_image_tag)"-base \
                -t "$(user_image_tag)" \
                -f "${FILES_DIR}/dockerfile.user" \
                "${FILES_DIR}"

            # Delete the intermediate layer. This should not affect the cache,
            # but is just meant to make things less confusing to the user.
            docker image rm "$(user_image_tag)"-base
        fi
    fi
}

# Parse the user config file, if found.
function parse_user_config () {
    if [ -f "$FILES_DIR"/user_config.json ]; then
        print_info "Loading user-specified docker arguments."
    fi
}

# Create the container if needed.
function maybe_create_dev_container() {
    # If we can't find a running container...
    if [[ $(docker ps -a -q -f name="$(container_name)" 2> /dev/null) == "" ]]; then
        # Allow a user to specify additional arguments through the user_config.json file,
        # if it exists.
        CONFIG_FILE="$FILES_DIR"/user_config.json

        if [ -f "$CONFIG_FILE" ]; then
            maybe_apt_install jq

            print_info "Loading user-specified docker arguments."

            docker_create_user_mounts=$(
                envsubst < "$CONFIG_FILE" | jq -r '.mounts | to_entries | .[] | "-v " + .key + ":" + .value'
            )

            docker_create_user_env_vars=$(
                envsubst < "$CONFIG_FILE" | jq -r '.env_vars | to_entries | .[] | "-e " + .key + "=" + .value'
            )

            docker_create_user_devices=$(
                envsubst < "$CONFIG_FILE" | jq -r '.devices | .[] | "--device " + .'
            )

            docker_create_user_flags=$(
                envsubst < "$CONFIG_FILE" | jq -r '.additional_flags | .[]'
            )
        fi

        docker_create_args=(
            "${docker_create_common_mounts[@]}"
            "${docker_create_common_devices[@]}"
            "${docker_create_common_env_vars[@]}"
            "${docker_create_common_flags[@]}"
            "${docker_create_user_mounts}"
            "${docker_create_user_env_vars}"
            "${docker_create_user_devices}"
            "${docker_create_user_flags}"
        )

        print_info "No container found. Creating"
        docker create ${docker_create_args[@]} --name "$(container_name)" "$(user_image_tag)"
    fi
}

# Start the container, then start a bash process within the container.
function start_container_and_enter() {
    # Start the container. No-op if already started.
    docker start "$(container_name)"
    # Now attach.
    docker attach "$(container_name)"
}


############ Parsing and execution ############

# Default run options for this script.
OPTION="run"

# Parse user input.
while (($# > 0)); do
    case $1 in
         -h | --help)
            print_usage_and_exit
            ;;
         down | delete | rebuild)
            if [[ ${OPTION} != "run" ]]; then
                print_error "Only one option is supported."
                exit 2
            fi
            OPTION=${1}
            ;;
         *) print_error "Unrecognized argument: ${1}"
            print_usage_and_exit
            ;;
    esac
    shift
done

# Options requiring us to bring the container down.
case "${OPTION}" in down | delete | rebuild )
    print_info "Killing development container."
    docker stop "$(container_name)" 2> /dev/null || true
esac

# Options requiring us to delete the container.
case ${OPTION} in delete | rebuild )
    print_info "Removing development container."
    docker rm "$(container_name)" 2> /dev/null || true
    print_info "Removing user image."
    docker image rm "$(user_image_tag)" 2> /dev/null || true
esac

# Options requiring us to force pull
case $OPTION in rebuild )
    # Force pull a new base image to build on. 
    pull_base_image 
esac

# Options requiring us to build/run a new container.
case $OPTION in rebuild | run )
    # User just wants to attach to a container (that may or may not be created yet)
    # and do their thing.
    if with_nvidia; then
        configure_nvidia
    fi
    maybe_build_user_image
    maybe_create_dev_container
    start_container_and_enter
esac
